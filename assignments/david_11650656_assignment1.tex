\documentclass{article}
\usepackage{morris}
\usepackage{enumerate}
\usepackage{diagbox}

\setcounter{section}{-1}
\renewcommand{\thesection}{Lecture \arabic{section}:}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{subsubsection}.}

\title{Reinforcement Learning -- Exercises Lectures 1-5}
\author{%
  Maurice Frank\\
  11650656\\
  \href{mailto:maurice.frank@posteo.de}{maurice.frank@posteo.de} \\
  Code: \href{https://github.com/morris-frank/uvadlc_practicals_2019/tree/master/assignment_2}{github}
}

\begin{document}
\maketitle

\section{}
\subsection{Linear algebra and multivariable derivatives}
\subsubsection{}
\begin{align}
  AB
  &= \bM{a_{11} & 0\\ 0 & a_{22}} \· \bM{b_{11} & b_{12}\\ b_{21} & b_{22}}\\
  &= \bM{a_{11}b_{11} & a_{11}b_{12}\\ a_{22}b_{21} & a_{22}b_{22}}\\
  AB^T
  &= \bM{a_{11} & 0\\ 0 & a_{22}} \· \bM{b_{11} & b_{21}\\ b_{12} & b_{22}}\\
  &= \bM{a_{11}b_{11} & a_{11}b_{21}\\ a_{22}b_{12} & a_{22}b_{22}}\\
  d^T B d
  &= \bM{d_1 & d_2} \bM{b_{11} & b_{12}\\ b_{21} & b_{22}} \bM{d_1\\d_2}\\
  &= d_1^2 b_{11} + d_1d_2 b_{12} + d_1d_2 b_{21} + d_2^2 b_{22}
\end{align}

\subsubsection{}
\begin{align}
  A^{-1}
  &= \bM{a_{11}^{-1} & 0 \\ 0 & a_{22}^{-1}}\\
  B^{-1}
  &= \÷{1}{b_{11}b_{22}-b_{21}b_{12}} \bM{b_{22} & -b_{21} \\ -b_{12} & b_{11}}
\end{align}

\subsubsection{}
\begin{align}
  \pf{c}{x}
  &= \bM{-2x \\ \÷{1}{yx}}\\
  \pf{c}{e}
  &= \bM{-2x & 1 \\ \÷{1}{yx} & -\ln{(x)}y^{-2}}
\end{align}

\subsubsection{}
\begin{align}
  f(\B{x})
  &= \Σ_i^N ix_i\\
  \pf{f(\B{x})}{\B{x}}
  &= \bM{1,\…,N}
\end{align}

\subsection{Probability theory}
\subsubsection{}
\begin{align}
  \E{[X + \α Y]}
  &= \E{[X]} + \α \E{[Y]}\\
  &= \μ + \α \ν
\end{align}

\subsubsection{}
\begin{align}
  \var{[X + \α Y]}
  &= \var{[X]} + \α^2 \var{[Y]} + 2\α \cov{[X, Y]}
\end{align}

\subsubsection{}
At last we have \(\σ^2\) which is just the variance of the noise in the measurements.
This is model independent (we can not train it away).
The bias term tells us how good our estimator estimates the sample data points.
The estimator variance tells us how jumpy our estimator is.
If the model has little parameters ('smooth') it will have high bias but low variance (if regularizes over the evident points but doesn't estimate the data that good anymore).
If the model is a complex one with high capacity it will have low bias but high variance.

\subsubsection{}
This is called the bias-variance trade-off as in machine learning we are mostly interested in building a model which has high bias and high variance.
The trade-off shows us that these two are opposite in their objective and that optimizing both is not easy.

\subsection{OLS, linear projection and gradient descent}
We have training set \(\B{X}\∈\ℝ^{n\× m}\) with targets \(\B{y}\∈\ℝ^n\).
We have a linear model \(f_{\B{\β}}(\B{X}) = \B{X}\·\B{\β}\).
\subsubsection{}
\[ \B{\β}\∈\ℝ^{m} \]

\subsubsection{}
\begin{align*}
  \hat{\B{\β}}
  &= \argmin_{\B{\β}} {(\B{y} - f_{\B{\β}}(\B{X}))}^2\\
  \pf{}{\hat{\B{\β}}}{(\B{y} - f_{\hat{\B{\β}}}(\B{X}))}^2
  &= \pf{}{\hat{\B{\β}}} {(\B{y} - \B{X}\hat{\B{\β}})}^2\\
  &= \pf{}{\hat{\B{\β}}} {\B{y}}^2 -2\B{y}\B{X}\hat{\B{\β}} +{(\B{X}\hat{\B{\β}})}^2\\
  &= 2\B{X}^T\B{X}\hat{\B{\β}} - 2\B{y}\B{X}\\
  &\hastoequal 0\\
  &\⇔\\
  \B{X}^T\B{X}\hat{\B{\β}}
  &= \B{X}^T\B{y}\\
  &\⇔\\
  \hat{\B{\β}}
  &= {(\B{X}^T\B{X})}^{-1}\B{X}^T\B{y}
\end{align*}

\subsubsection{}
\begin{align*}
  \B{\ε}_{\B{\β}}
  &= \B{y} - \B{X}\hat{\B{\β}}
\end{align*}
\B{SKIPPED}

\subsubsection{}
\B{SKIPPED}
\subsubsection{}
\B{SKIPPED}
\subsubsection{}
\B{SKIPPED}
\subsubsection{}
\B{SKIPPED}

\section{Introduction}
\subsection{Introduction}
\subsubsection{}
The curse of dimensionality are mutliple sad observation called one will make when working with high-dimensional data.
In general the problems arise from the fact that the number of value combinations rises exponentially, with the dimension in the exponential.
E.g. in hyper-parameter optimization using grid-search the number of needed models to be tested rises exponentially with the number of hyper-parameters.
Another example we often see in machine learning with high-dimensional data.
With a limited number of trainings samples the distribution of those might be highly sparse in its space akin like a set of dirac functions.
Trying to approximate that might be difficult.

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    N_{\text{states}}
    &= N_{\text{predator states}} \· N_{\text{prey states}}\\
    &= 5^2\·5^2\\
    &= 625
  \end{align*}

  \item
  As it is a toroid we just have to remember the differences of the two entities.
  So the state is just the offset in toroidial coordinates.

  \item
  \begin{align*}
    N^{'}_{\text{states}}
    &= 5\·5\\
    &= 25
  \end{align*}

  \item
  The advantage of this approach is that we have fewer states and now multiple states that have to learn the same response to it.
  Thus we can assume faster training of our predator.

  \item
  For Tic-Tac-Toe we could reduce the state space by using the point symmetry of the game board.
  Of all starting states that are symmetric through the center only keep one.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  The greedy agent will perform better.
  Tic-tac-toe is a solved games as such a trained agent can know the perfect move to any situation and no exploration is necessary.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  We decrease the exploration probability \(\ε\) each step with a discount factor \(\η\).
  We can write the exploration probability at step \(\ε_t\) with:
  \begin{align*}
    \ε_t
    &= \ε\·\η^t
  \end{align*}

  \item
  No, that method would not work if the opponent changes strategy.
  It continously decreases exploration over time independent of the game dynamics.
  We can adapt our strategy by introducing the time step of the last strategy change of the opponent \(t_{\text{change}}\).
  Then we restart from the beginning if the strategy changes:
  \begin{align*}
    \ε_t
    &= \ε\·\η^{t - t_{\text{change}}}
  \end{align*}
\end{enumerate}

\subsection{Exploration}
\subsubsection{}
\begin{align*}
  (1-\ε) + \÷{\ε}{n}
\end{align*}

\subsubsection{}
\(A_3\) and \(A_4\).
The first one could be greedy as all states have the same average.
Same for the second as 2 and 3 have the greedy average.
The third action could be greedy as state 2 has the top average then.
The next two are suboptimal thus have to be exploration.

\subsubsection{}
\(R_0 = -1\) and \(R_1 = +1\). By random we choose \(A_0\) in the first step.
See the development of the Q-values (bold is the chooses action with greedy policy):

\begin{center}
  \begin{tabular}{ccccc}
    step & \(Q_0^{\text{pessi}}\) & \(Q_1^{\text{pessi}}\) & \(Q_0^{\text{opti}}\) & \(Q_1^{\text{opti}}\) \\\toprule
    0 & -5 & -5 & 5 & 5\\
    1 & \B{-1} & -5 & \B{-1} & 5\\
    2 & \B{-1} & -5 & -1 & \B{1}\\
    3 & \B{-1} & -5 & -1 & \B{1}\\
  \end{tabular}
\end{center}

\subsubsection{}
The optimistic initalization leads to the higher return (\(== 1\)) than with the pessimistic initalization (\(== -3\)).
If broken the tie the other way:
\begin{center}
  \begin{tabular}{ccccc}
    step & \(Q_0^{\text{pessi}}\) & \(Q_1^{\text{pessi}}\) & \(Q_0^{\text{opti}}\) & \(Q_1^{\text{opti}}\) \\\toprule
    0 & -5 & -5 & 5 & 5\\
    1 & -5 & \B{1} & 5 & \B{1}\\
    2 & -5 & \B{1} & \B{-1} & 1\\
    3 & -5 & \B{1} & -1 & \B{1}\\
  \end{tabular}
\end{center}
In this case the pessimistic initalization lead to the higher return (\(==3\)) than the optimistic initalization (\(==1\)).


\subsubsection{}
The optimistic initalization leads to the better estimate of the Q-values.

\subsubsection{}
The optimistic initalization works better for exploration as its basic assumption is that any action could be the best until proven otherwise.
As such it will lead to everything being tried using the high initalization values.

\section{MDPs and dynamic programming}
\subsection{Markov Decision Processes}
\subsubsection{}
\begin{enumerate}[(a)]
  \item Description of the games defined in Section 1.2 in Sutton/Barton:
  \begin{center}
    \hspace*{-3cm}\begin{tabular}{p{3cm}p{4cm}p{4cm}p{4cm}}
      Game & State Space & Action Space & Reward Signal \\\toprule
      Master chess & All possible chess game sequences & Set of single legal moves in chess & \\\midrule
      Adaptive petroleum refinery controller & Current state of the refinery (e.g. yield, quality, fillness) + state of the marginal costs & Yield, cost and quality levers & RoI \\\midrule
      Newborn gazelle & basically all possible worlds around the gazelle & any physical action possible as a newborn gazelle & health and joy \\\midrule
      Trash robot & physical state, power level, history of movement trajectories & possible movements & How much trash taken + not loosing all charge \\\bottomrule
    \end{tabular}
  \end{center}

  \item Another example of an Reinforcement learning application:
  \hfill
  \begin{center}
    \hspace*{-3cm}\begin{tabular}{p{3cm}p{4cm}p{4cm}p{4cm}}
      Game & State space & Action space & Reward signal \\\toprule
      Composing musician & all possible partly composed songs & adding new notes / instruments & beauty of the song \\\bottomrule
    \end{tabular}
  \end{center}

  \item An example for a game that is hard to represent with an MDP is Age of Empires with Fog of War set to on. As the fog of war hides the gambe boards current state to the actor we can not actually build a state space that describes the games state.

  \item One could think of a maze with parts that only can be overcome with enough stamina. In this case stamina would be a state variable.

  \item With these three actions we can reach any state the robot might get into as such they are sufficient actions for the game. The disadvantage though is that they are quite low level actions for the robot. It might be more usefull for the RL algorithm to focus on the high-level control decision. Actually driving the robot from A to B can be solved otherwise.

  \item One could separate the two problems, driving and decision making. Solve both with their own RL algorithm.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    G &= \Σ_{i=0}^N \γ^i \· R_{i}
  \end{align*}

  \item
  \begin{align*}
    \Σ_{k=0}^\infty \γ^k &= 1 + \γ\·\Σ_{k=0}^\infty \γ^k\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k - \γ\·\Σ_{k=0}^\infty \γ^k &= 1\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k \· (1-\γ) &= 1\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k &= \÷{1}{1-\γ}\\
  \end{align*}

  \item
  Because we made the task episodic we assumed the run through the maze to be always the same length.
  Thus our robot does not focus on learning to solve the maze in shorter time lengths.

  \item
  A discount factor of \(\γ<1\) would help with this because it would discount the return of solving the maze more the longer the robot needed to exit it.
  Thus the robot will learn to solve it faster.

  \item
  We could add a negative reward for each done step (moving is hard!).
  To avoid the negative reward the robot would learn to do fewer steps to solve the maze.
\end{enumerate}

\subsection{Homework: Dynamic Programming}
\subsubsection{}
First the  stochastic case:
\begin{align*}
  v^{\π}(s)
  &= \Σ_a \π(a|s) \Σ_{s',r} p(s',r|s,a)\left[r+ \γ\·v_{\π}(s')\right]\\
  &= \Σ_a \π(a|s) \· q^\π(s,a)
\end{align*}

and the deterministic policy:
\begin{align*}
  v^{\π}(s)
  &= \Σ_{s',r} p(s',r|s,\π(s))\left[r+ \γ\·v_{\π}(s')\right]\\
  &= q^\π(s,\π(s))
\end{align*}

\subsubsection{}
See Algorithm~\ref{alg:policy_evaluation}.
\begin{algorithm}
  \caption{Policy Evaluation}%
  \label{alg:policy_evaluation}
  \begin{algorithmic}
    \Repeat
      \State \(\Δ \gets 0\)
      \ForAll{\(s\∈ S\)}
        \ForAll{\(a\∈ A(s)\)}
          \State \(q \gets Q(s, a)\)
          \State \(Q(s, a) \gets \Σ_{s',r} p(s',r|s,a)[r+\γ\·Q(s',\π(s'))]\)
          \State \(\Δ \gets \max{(\Δ, |q-Q(s,a)|)}\)
        \EndFor
      \EndFor
    \Until{\(\Δ < \θ\)}
  \end{algorithmic}
\end{algorithm}

\subsubsection{}
See Algorithm~\ref{alg:policy_improvement}.
\begin{algorithm}[!h]
  \caption{Policy Improvement}%
  \label{alg:policy_improvement}
  \begin{algorithmic}
    \State \(\text{policy-stable} \gets true\)
    \ForAll{\(s \∈ S\)}
      \State \(\text{old-action} \gets \π(s)\)
      \State \(\π(s) \gets \argmax_a Q(s,a)\)
      \If{\(\text{old-action} \neq \π(s)\)}
        \State \(\text{policy-stable} \gets false\)
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

\subsubsection{}
\begin{align*}
    q_{k+1}(s,a)
    &= \Σ_{s',r} p(s',r|s,a) \left[r + \γ\·\max_{a'}q_k(s',a')\right]
\end{align*}

\section{Monte Carlo methods}
\subsection{Homework: Monte carlo}
\subsubsection{}
\begin{enumerate}[(a)]
  \item With first-visit MC:
  \begin{align*}
    v(s_0)
    &= 5\·\÷{1}{3} [\γ^2 + \γ^4 + \γ^3]\\
    &= 3.6585
  \end{align*}

  \item With every-visit MC:
  \begin{align*}
    v(s_0)
    &= 5\·\÷{1}{12} [\γ^2 + \γ + 1 + \γ^4 + \γ^3 + \γ^2 + \γ + 1 + \γ^3 + \γ^2 + \γ + 1]\\
    &= 4.268
  \end{align*}
\end{enumerate}

\subsubsection{}
The problem with \I{ordinary importance sampling} in off-policy Monte Carlo is that the variance of its estimation is unbounded.
Thus if the variance of the observed return is high (to infinite) the variance of the value estimation also get extremely high.
This can considerably slow down convergence of the value estimation.

\subsubsection{}
The problem with \I{weighted importance sampling} is that its value estimation is biased.
The expectation of the biased with having \(v_b(s)\) instead of the wanted \(v_\π(s)\).
Because of this the trajectory will be of the behavior policy not the target policy.
The bias does go asymptotically to zero though with longer trajectories.
Thus it is only a problem with little number of trajectories.

\section{Temporal difference methods}
\subsection{Temporal difference learning (Application)}
\subsubsection{}
\begin{enumerate}[(a)]
  \item TD(0)
  \begin{align*}
    V(s_{t-1}) &\gets V(s_{t-1}) + \α\·(r_t + \γ V(s_t) - V(s_{t-1}))
  \end{align*}
  \begin{center}
    \begin{tabular}{lll}
      t & A & B\\\toprule
      0 & 0 & 0\\
      1 & -0.3 & 0\\
      2 & -0.3 & 0.37\\
      3 & -0.7 & 0.37\\
      4 & -0.893 & 0.37\\
      5 & -0.893 & 0.3437\\\bottomrule
    \end{tabular}
  \end{center}

  \item 3-step TD
  \begin{center}
    \begin{tabular}{ccc}
      t & A & B\\\toprule
    \end{tabular}
  \end{center}
  \B{SKIPPED}

  \item SARSA
  \begin{align*}
    Q(s_t,a_t) &\gets Q(s_t,a_t) + \α[r_t + Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)]
  \end{align*}
  \begin{center}
    \begin{tabular}{lllll}
      t & A,1 & A,2 & B,1 & B,2\\\toprule
      0 & 0 & 0 & 0 & 0 \\
      1 & -0.3 & 0 & 0 & 0 \\
      2 & -0.3 & 0 & 0.4 & 0 \\
      3 & -0.3 & -0.43 & 0.4 & 0 \\
      4 & -0.63 & -0.43 & 0.4 & 0 \\
      5 & -0.63 & -0.43 & 0.4 & 0.1 \\\bottomrule
    \end{tabular}
  \end{center}

  \item Q-Learning
  \begin{align*}
    Q(s_t,a_t) &\gets Q(s_t,a_t) + \α(r_{t+1} + \γ\max_a Q(s_{t+1}, a) - Q(s_t,a_t))
  \end{align*}
  \begin{center}
    \begin{tabular}{lllll}
      t & A,1 & A,2 & B,1 & B,2\\\toprule
      0 & 0 & 0 & 0 & 0\\
      1 & -0.3 & 0 & 0 & 0\\
      2 & -0.3 & 0 & 0.4 & 0\\
      3 & -0.3 & -0.4 & 0.4 & 0\\
      4 & -0.53 & -0.4 & 0.4 & 0\\
      5 & -0.53 & -0.4 & 0.4 & 0.1\\\bottomrule
    \end{tabular}
  \end{center}
\end{enumerate}

\subsubsection{}
A better policy would be to take action 1 for B and action 2 for A.
This is just using the max Q values from the Q-learning estimation.

\subsubsection{}
\begin{enumerate}
  \item The value estimate of \(\π_{\text{random}}\) would reflect the actual correct values while the \(\π_{\text{student}}\) policy would show a skewed image.
  For B it always picks the better off terminal action while valueing action 1 with zero.
  For A it would get stuck in a loop of worsening scores.

  \item The problem with a fixed random policy is that it does not learn to avoid obvious wrong decision (like 2 for A).
  The problem with the alternative policy is that it was pre-emptively set from a wrong approximation of the correct values.
  So it is stuck not discovering correct alternative strategies.

  \item An \(\ε\)-greedy would be beneficial in this case as it would allow our policy to make definitly good decisions (like 2 for B) but if set still allows to see alternative options.
  In this case the student policy would be able to see the value of taking action 1 for A.
\end{enumerate}

\subsection{Contraction mapping}
\subsubsection{}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    T(x)
    &:= 1+\÷{1}{3}x\\
    &\⇒\\
    x &= 1+\÷{1}{3}x\\
    &\⇔\\
    x_{\text{fix}}
    &= \÷{1}{1-\÷{1}{3}}\\
    &= \÷{3}{2}\\
  \end{align*}

  \item
  \begin{align*}
    (Tf)(s) &:= \÷{-1}{2}f(s) + g(s)\\
  \end{align*}
  \B{SKIPPED}
\end{enumerate}

\subsubsection{}
\B{SKIPPED}

\subsection{Temporal difference learning (theory)}
\subsubsection{}
\begin{align*}
  \÷{1}{M}\Σ_{n=1}^M G_n(s) &= V_{M-1}(S) + \α_M[G_M(s) - V_{M-1}(S)]\\
  \÷{1}{M}G_M(S) + \÷{M-1}{M}V_{M-1}(S) &=  V_{M-1}(S) + \α_M[G_M(s) - V_{M-1}(S)]\\
  \÷{1}{M}G_M(S) - \÷{1}{M}V_{M-1}(S) &= \α_M[G_M(s) - V_{M-1}(S)]\\
  \÷{\÷{1}{M}(G_M(S) - V_{M-1}(S))}{G_M(s) - V_{M-1}(S)} &= \α_M\\
  \÷{1}{M} &= \α_M
\end{align*}

\subsubsection{}
\begin{align*}
  \δ_t &= R_{t+1} + \γ V(S_{t+1}) - V(S_t)\\
\end{align*}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    \E[\δ_t|S_t =s]
  \end{align*}
  \B{SKIPPED}

  \item
  \begin{align*}
    \E[\δ_t|S_t =s,A_t = a]
  \end{align*}
  \B{SKIPPED}
\end{enumerate}

\subsubsection{}

\subsection{Homework: Maximization bias}
\subsubsection{}
\begin{center}
  \B{Q-learning}\\
  \begin{tabular}{r|cccc}
    \diagbox{S}{A} & 0 & 1 & 2 & 3\\\hline
    A & 2 & 1.5 & --- & ---\\
    B & 1 & 1 & 2 & 0 \\
  \end{tabular}
\end{center}

\begin{center}
  \B{SARSA}\\
  \begin{tabular}{r|cccc}
    \diagbox{S}{A} & 0 & 1 & 2 & 3\\\hline
    A & 2/1 & 1.5 & --- & ---\\
    B & 1 & 1 & 2 & 0 \\
  \end{tabular}
\end{center}

SARSA has 2 or 1 in A,0 depending on the Policy used. For a random policy it would have 1 but with a greedy 2. (With \(\ε\)-greedy around 2).

\subsubsection{}
Both Q-learning and SARSA suffer from Maximization Bias.
This bias happens as both learning methods use both a maximum function inside their  Q update.
For SARSA this comes from the policy (e.g. \(\ε\)-greedy) and in Q-learning the maximum is directly used.
The effect is visible when rewards associated with a state have a outlier maximum, then the Q estimation will focus mainly on this maximum.
In our case we see the effects of this on the left action (0) of state A.
As the maximum of rewards after B is 2 in our observed data we will see this action being biased towards 2 in the beginning of training.

\subsubsection{}
The problem with our argmax methods is that we are using the maximum of our estimates to choose the maximizing action.
As we are updating the Q values while using them to find the maximum Q value we introduce the bias that focusses on the maximum.
The idea of Double Q-learning now is to have to set of Q values, one to to choose the maximizing action and one to actually determine the value of the maximum.
So we have two set of Q values. When we update a value of one of them we use the argmax on those Q values but then retrieve the actual Q value from the second set of Q values.

(Somehow this reminds me of how a Vickroy auction works)


\subsubsection{}
\begin{center}
  \begin{tabular}{r|cccc}
    \diagbox{S}{A} & 0 & 1 & 2 & 3\\\hline
    A & 1 & 1.5 & --- & ---\\
    B & 1 & 1 & 1 & 1 \\
  \end{tabular}
\end{center}

\end{document}
