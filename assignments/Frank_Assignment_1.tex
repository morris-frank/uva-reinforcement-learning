\documentclass{article}
\usepackage{morris}
\usepackage{enumerate}

\setcounter{section}{-1}
\renewcommand{\thesection}{Lecture \arabic{section}:}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{subsubsection}.}

\title{Reinforcement Learning - Exercises Lectures 1-5}
\author{%
  Maurice Frank\\
  11650656\\
  \href{mailto:maurice.frank@posteo.de}{maurice.frank@posteo.de} \\
  Code: \href{https://github.com/morris-frank/uvadlc_practicals_2019/tree/master/assignment_2}{github}
}

\begin{document}
\maketitle

\section{}
\subsection{Linear algebra and multivariable derivatives}
\subsubsection{}
\begin{align}
  AB
  &= \bM{a_{11} & 0\\ 0 & a_{22}} \· \bM{b_{11} & b_{12}\\ b_{21} & b_{22}}\\
  &= \bM{a_{11}b_{11} & a_{11}b_{12}\\ a_{22}b_{21} & a_{22}b_{22}}\\
  AB^T
  &= \bM{a_{11} & 0\\ 0 & a_{22}} \· \bM{b_{11} & b_{21}\\ b_{12} & b_{22}}\\
  &= \bM{a_{11}b_{11} & a_{11}b_{21}\\ a_{22}b_{12} & a_{22}b_{22}}\\
  d^T B d
  &= \bM{d_1 & d_2} \bM{b_{11} & b_{12}\\ b_{21} & b_{22}} \bM{d_1\\d_2}\\
  &= d_1^2 b_{11} + d_1d_2 b_{12} + d_1d_2 b_{21} + d_2^2 b_{22}
\end{align}

\subsubsection{}
\begin{align}
  A^{-1}
  &= \bM{a_{11}^{-1} & 0 \\ 0 & a_{22}^{-1}}\\
  B^{-1}
  &= \÷{1}{b_{11}b_{22}-b_{21}b_{12}} \bM{b_{22} & -b_{21} \\ -b_{12} & b_{11}}
\end{align}

\subsubsection{}
\begin{align}
  \pf{c}{x}
  &= \bM{-2x \\ \÷{1}{yx}}\\
  \pf{c}{e}
  &= \bM{-2x & 1 \\ \÷{1}{yx} & -\ln{(x)}y^{-2}}
\end{align}

\subsubsection{}
\begin{align}
  f(\B{x})
  &= \Σ_i^N ix_i\\
  \pf{f(\B{x})}{\B{x}}
  &= \bM{1,\…,N}
\end{align}

\subsection{Probability theory}
\subsubsection{}
\begin{align}
  \E{[X + \α Y]}
  &= \E{[X]} + \α \E{[Y]}\\
  &= \μ + \α \ν
\end{align}

\subsubsection{}
\begin{align}
  \var{[X + \α Y]}
  &= \var{[X]} + \α^2 \var{[Y]} + 2\α \cov{[X, Y]}
\end{align}

\subsubsection{}
At last we have \(\σ^2\) which is just the variance of the noise in the measurements.
This is model independent (we can not train it away).
The bias term tells us how good our estimator estimates the sample data points.
The estimator variance tells us how jumpy our estimator is.
If the model has little parameters ('smooth') it will have high bias but low variance (if regularizes over the evident points but doesn't estimate the data that good anymore).
If the model is a complex one with high capacity it will have low bias but high variance.

\subsubsection{}
This is called the bias-variance trade-off as in machine learning we are mostly interested in building a model which has high bias and high variance.
The trade-off shows us that these two are opposite in their objective and that optimizing both is not easy.

\subsection{OLS, linear projection and gradient descent}
We have training set \(\B{X}\∈\ℝ^{n\× m}\) with targets \(\B{y}\∈\ℝ^n\).
We have a linear model \(f_{\B{\β}}(\B{X}) = \B{X}\·\B{\β}\).
\subsubsection{}
\[ \B{\β}\∈\ℝ^{m} \]

\subsubsection{}
\begin{align*}
  \hat{\B{\β}}
  &= \argmin_{\B{\β}} {(\B{y} - f_{\B{\β}}(\B{X}))}^2\\
  \pf{}{\hat{\B{\β}}}{(\B{y} - f_{\hat{\B{\β}}}(\B{X}))}^2
  &= \pf{}{\hat{\B{\β}}} {(\B{y} - \B{X}\hat{\B{\β}})}^2\\
  &= \pf{}{\hat{\B{\β}}} {\B{y}}^2 -2\B{y}\B{X}\hat{\B{\β}} +{(\B{X}\hat{\B{\β}})}^2\\
  &= 2\B{X}^T\B{X}\hat{\B{\β}} - 2\B{y}\B{X}\\
  &\hastoequal 0\\
  &\⇔\\
  \B{X}^T\B{X}\hat{\B{\β}}
  &= \B{X}^T\B{y}\\
  &\⇔\\
  \hat{\B{\β}}
  &= {(\B{X}^T\B{X})}^{-1}\B{X}^T\B{y}
\end{align*}

\subsubsection{}
\begin{align*}
  \B{\ε}_{\B{\β}}
  &= \B{y} - \B{X}\hat{\B{\β}}
\end{align*}

\subsubsection{}
\subsubsection{}
\subsubsection{}
\subsubsection{}

\section{Introduction}
\subsection{Introduction}
\subsubsection{}
The curse of dimensionality are mutliple sad observation called one will make when working with high-dimensional data.
In general the problems arise from the fact that the number of value combinations rises exponentially, with the dimension in the exponential.
E.g. in hyper-parameter optimization using grid-search the number of needed models to be tested rises exponentially with the number of hyper-parameters.
Another example we often see in machine learning with high-dimensional data.
With a limited number of trainings samples the distribution of those might be highly sparse in its space akin like a set of dirac functions.
Trying to approximate that might be difficult.

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    N_{\text{states}}
    &= N_{\text{predator states}} \· N_{\text{prey states}}\\
    &= 5^2\·5^2\\
    &= 625
  \end{align*}

  \item
  As it is a toroid we just have to remember the differences of the two entities.
  So the state is just the offset in toroidial coordinates.

  \item
  \begin{align*}
    N^{'}_{\text{states}}
    &= 5\·5\\
    &= 25
  \end{align*}

  \item
  The advantage of this approach is that we have fewer states and now multiple states that have to learn the same response to it.
  Thus we can assume faster training of our predator.

  \item
  For Tic-Tac-Toe we could reduce the state space by using the point symmetry of the game board.
  Of all starting states that are symmetric through the center only keep one.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  The greedy agent will perform better.
  Tic-tac-toe is a solved games as such a trained agent can know the perfect move to any situation and no exploration is necessary.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  We decrease the exploration probability \(\ε\) each step with a discount factor \(\η\).
  We can write the exploration probability at step \(\ε_t\) with:
  \begin{align*}
    \ε_t
    &= \ε\·\η^t
  \end{align*}

  \item
  No, that method would not work if the opponent changes strategy.
  It continously decreases exploration over time independent of the game dynamics.
  We can adapt our strategy by introducing the time step of the last strategy change of the opponent \(t_{\text{change}}\).
  Then we restart from the beginning if the strategy changes:
  \begin{align*}
    \ε_t
    &= \ε\·\η^{t - t_{\text{change}}}
  \end{align*}
\end{enumerate}

\subsection{Exploration}
\subsubsection{}
\begin{align*}
  (1-\ε) + \÷{\ε}{n}
\end{align*}

\subsubsection{}
\(A_3\) and \(A_4\).
The first one could be greedy as all states have the same average.
Same for the second as 2 and 3 have the greedy average.
The third action could be greedy as state 2 has the top average then.
The next two are suboptimal thus have to be exploration.

\subsubsection{}
\(R_0 = -1\) and \(R_1 = +1\). By random we choose \(A_0\) in the first step.
See the development of the Q-values (bold is the chooses action with greedy policy):

\begin{center}
  \begin{tabular}{ccccc}
    step & \(Q_0^{\text{pessi}}\) & \(Q_1^{\text{pessi}}\) & \(Q_0^{\text{opti}}\) & \(Q_1^{\text{opti}}\) \\\toprule
    0 & -5 & -5 & 5 & 5\\
    1 & \B{-1} & -5 & \B{-1} & 5\\
    2 & \B{-1} & -5 & -1 & \B{1}\\
    3 & \B{-1} & -5 & -1 & \B{1}\\
  \end{tabular}
\end{center}

\subsubsection{}
The optimistic initalization leads to the higher return (\(== 1\)) than with the pessimistic initalization (\(== -3\)).
If broken the tie the other way:
\begin{center}
  \begin{tabular}{ccccc}
    step & \(Q_0^{\text{pessi}}\) & \(Q_1^{\text{pessi}}\) & \(Q_0^{\text{opti}}\) & \(Q_1^{\text{opti}}\) \\\toprule
    0 & -5 & -5 & 5 & 5\\
    1 & -5 & \B{1} & 5 & \B{1}\\
    2 & -5 & \B{1} & \B{-1} & 1\\
    3 & -5 & \B{1} & -1 & \B{1}\\
  \end{tabular}
\end{center}
In this case the pessimistic initalization lead to the higher return (\(==3\)) than the optimistic initalization (\(==1\)).


\subsubsection{}
The optimistic initalization leads to the better estimate of the Q-values.

\subsubsection{}
The optimistic initalization works better for exploration as its basic assumption is that any action could be the best until proven otherwise.
As such it will lead to everything being tried using the high initalization values.

\section{MDPs and dynamic programming}
\subsection{Markov Decision Processes}
\subsubsection{}
\begin{enumerate}[(a)]
  \item Description of the games defined in Section 1.2 in Sutton/Barton:
  \begin{center}
    \hspace*{-3cm}\begin{tabular}{p{3cm}p{4cm}p{4cm}p{4cm}}
      Game & State Space & Action Space & Reward Signal \\\toprule
      Master chess & All possible chess game sequences & Set of single legal moves in chess & \\\midrule
      Adaptive petroleum refinery controller & Current state of the refinery (e.g. yield, quality, fillness) + state of the marginal costs & Yield, cost and quality levers & RoI \\\midrule
      Newborn gazelle & basically all possible worlds around the gazelle & any physical action possible as a newborn gazelle & health and joy \\\midrule
      Trash robot & physical state, power level, history of movement trajectories & possible movements & How much trash taken + not loosing all charge \\\bottomrule
    \end{tabular}
  \end{center}

  \item Another example of an Reinforcement learning application:
  \hfill
  \begin{center}
    \hspace*{-3cm}\begin{tabular}{p{3cm}p{4cm}p{4cm}p{4cm}}
      Game & State space & Action space & Reward signal \\\toprule
      Composing musician & all possible partly composed songs & adding new notes / instruments & beauty of the song \\\bottomrule
    \end{tabular}
  \end{center}

  \item An example for a game that is hard to represent with an MDP is Age of Empires with Fog of War set to on. As the fog of war hides the gambe boards current state to the actor we can not actually build a state space that describes the games state.

  \item One could think of a maze with parts that only can be overcome with enough stamina. In this case stamina would be a state variable.

  \item With these three actions we can reach any state the robot might get into as such they are sufficient actions for the game. The disadvantage though is that they are quite low level actions for the robot. It might be more usefull for the RL algorithm to focus on the high-level control decision. Actually driving the robot from A to B can be solved otherwise.

  \item One could separate the two problems, driving and decision making. Solve both with their own RL algorithm.
\end{enumerate}

\subsubsection{}
\begin{enumerate}[(a)]
  \item
  \begin{align*}
    G &= \Σ_{i=0}^N \γ^i \· R_{i}
  \end{align*}

  \item
  \begin{align*}
    \Σ_{k=0}^\infty \γ^k &= 1 + \γ\·\Σ_{k=0}^\infty \γ^k\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k - \γ\·\Σ_{k=0}^\infty \γ^k &= 1\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k \· (1-\γ) &= 1\\
    &\⇔\\
    \Σ_{k=0}^\infty \γ^k &= \÷{1}{1-\γ}\\
  \end{align*}

  \item
  Because we made the task episodic we assumed the run through the maze to be always the same length.
  Thus our robot does not focus on learning to solve the maze in shorter time lengths.

  \item
  A discount factor of \(\γ<1\) would help with this because it would discount the return of solving the maze more the longer the robot needed to exit it.
  Thus the robot will learn to solve it faster.

  \item
  We could add a negative reward for each done step (moving is hard!).
  To avoid the negative reward the robot would learn to do fewer steps to solve the maze.
\end{enumerate}

\subsection{Homework: Dynamic Programming}
\subsubsection{}
First the  stochastic case:
\begin{align*}
  v^{\π}(s)
  &= \Σ_a \π(a|s) \Σ_{s',r} p(s',r|s,a)\left[r+ \γ\·v_{\π}(s')\right]\\
  &= \Σ_a \π(a|s) \· q^\π(s,a)
\end{align*}

and the deterministic policy:
\begin{align*}
  v^{\π}(s)
  &= \argmax_a \π(a|s) \Σ_{s',r} p(s',r|s,a)\left[r+ \γ\·v_{\π}(s')\right]\\
  &= \argmax_a \π(a|s) \· q^\π(s,a)
\end{align*}

\subsubsection{}


\subsubsection{}

\subsubsection{}

\section{Monte Carlo methods}
\subsection{Homework: Monte carlo}
\subsubsection{}
\subsubsection{}
\subsubsection{}

\section{Temporal difference methods}
\subsection{Temporal difference learning (Application)}
\subsubsection{}
\subsubsection{}
\subsubsection{}

\subsection{Contraction mapping}
\subsubsection{}
\subsubsection{}

\subsection{Temporal difference learning (theory)}
\subsubsection{}
\subsubsection{}
\subsubsection{}

\subsection{Homework: Maximization bias}
\subsubsection{}
\subsubsection{}
\subsubsection{}
\subsubsection{}

\end{document}
